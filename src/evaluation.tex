\xchapter{Evaluation}{}
\label{cap:evaluation}

In this chapter, we evaluate JVMTIPROF by using it to instrument Elasticsearch. A profile-guided frequency scaling solution~\cite{hurryupccgrid} is adapted to use JVMTIPROF instead of JVMTI. We then measure the overhead and identify whether JVMTIPROF can produce the same gains as the baseline despite its abstractions.

\section{Experimental Setup}

We conduct experiments on a bare-metal server instance provided by the Chameleon Cloud service. The server consists of an Intel Xeon Gold 6126 (Skylake architecture) with 196GB of DRAM and a 220GB SSD disk. We run Ubuntu 20.04 (Linux kernel 5.4) and Elasticsearch version 6.5.4. For the search index database, we use the Wikipedia dump version \code{enwiki-latest-pages-articles} downloaded in September 2022.

The CPU has 24 physical cores equally divided between two sockets. Intel's Hyperthreading technology is turned off, so we only consider the physical core count. To isolate the network effects in the shared experimental platform, we configure socket 0 to run the server-side applications (Elasticsearch) and socket 1 to execute the load generator (FABAN).

Energy measurement is done through Intel's Running Average Power Limit (RAPL) interface. We measure the energy consumption counter at the beginning and end of the experiment, and the difference between those two values is considered the energy consumed for that particular experiment.

The client, FABAN~\cite{faban}, is responsible for generating query requests with randomized keywords, ranging from 1 to 18 keywords, under a Zipfian distribution.

The load generation process consisted of 20-minute runs of continuous search queries. The FABAN workload generator is used to create four clients, with each client issuing, on average, one request per second. The goal is to analyze the behavior of each solution, considering a single request at a time. An additional configuration of three times the requests-per-seconds is used to explore loads similar to that of production environments.

We perform six runs for each combination of load and solution and report the mean and standard deviation of the 99-percentile latency and energy consumption.

\section{Baseline}

We use the Linux Ondemand power governor and Hurryup~\cite{hurryupccgrid} as baselines for our experiments. Hurryup is a profile-guided frequency scaling solution for Elasticsearch that outperforms Ondemand on energy consumption while attaining acceptable levels of tail latency constraints.

Hurryup works by instrumenting Elasticsearch's hot functions to produce enter and exit events. A runtime scheme uses these events to adapt the individual processor core frequencies. In our experiments, we observe Hurryup saving up to 25\% in energy compared to Linux's Ondemand governor, as seen in Figure~\ref{fig:ondem_vs_hup}.

\begin{figure}[hb]
\centering
\includegraphics[width=1.0\textwidth]{src/figure/ondem_vs_hup.png}
\caption{An experiment comparing Linux's Ondemand governor against Hurryup's frequency scaling scheme. The left plot presents Elasticsearch's service time, and the right plot the energy consumption of the CPU while executing search requests. Results are shown for both a low and high server load.}
\label{fig:ondem_vs_hup}
\end{figure}

\section{Results}

We modify Hurryup to use JVMTIPROF's instrumentation methods and re-execute the experiments as reported in Figure~\ref{fig:ondem_vs_hup_vs_newhup}. We notice that JVMTIPROF can be similar to or slightly worse than JVMTI in the context of Hurryup but can still achieve energy gains compared to Ondemand.

\begin{figure}[ht]
\centering
\includegraphics[width=1.0\textwidth]{src/figure/ondem_vs_hup_vs_newhup.png}
\caption{An experiment comparing Hurryup's performance when implemented with JVMTIPROF versus the baseline. The left plot presents Elasticsearch's service time, and the right plot the energy consumption of the CPU while executing search requests. Results are shown for both a low and high server load.}
\label{fig:ondem_vs_hup_vs_newhup}
\end{figure}

At a lower load, the JVMTI and JVMTIPROF implementations do not present a statistically significant difference in service time and energy consumption. When tripling the load, the JVMTIPROF version can be 6\% worse in service time and 2.78\% worse in energy consumption.

We also compare the overhead of instrumenting Elasticsearch's hot functions, excluding Hurryup-specific logic. Results are presented in Figure~\ref{fig:overhead}. There is no statistically significant difference between instrumenting or not instrumenting said functions with either JVMTI or JVMTIPROF.

\begin{figure}[ht]
\centering
\includegraphics[width=1.0\textwidth]{src/figure/overhead.png}
\caption{An experiment comparing the performance of Elasticsearch when their hot functions are kept as-is or hooked with JVMTI and JVMTIPROF. The left plot presents Elasticsearch's service time, and the right plot the energy consumption of the CPU while executing search requests. Results are shown for both a low and high server load.}
\label{fig:overhead}
\end{figure}

These results also show that the context in which the instrumentation is used may cause performance differences. When instrumenting to run Hurryup at a higher load, JVMTIPROF performed worse than JVMTI, but no difference is seen at lower loads or with scheduling logic removed.
